---
layout: default
title: "Início"
---

# Bem-vindo ao meu site 

#"Attention, a very simple explanation" 
##Learn attention with intuition, simple examples, and basic math!
In 2017, Google researchers introduced the Transformer model in their now-famous paper "Attention Is All You Need." This new approach marked a turning point in the field of natural language processing (NLP). By rethinking how machines process language, it quickly outperformed earlier models across key benchmarks such as machine translation, text classification, text generation, and question answering. Its success laid the groundwork for nearly all modern NLP systems.
##1. Bidirectional models
When we read a sentence, we don't usually process each word on its own. Instead, we grasp meaning by looking at how words connect to those around them. The context that comes before and after a word often changes its sense completely, and this reliance on surrounding words is what makes language so rich and flexible.
This ability to look at both sides of the sentence is exactly what defines bidirectional models such as BERT and many derived models. By incorporating information from both the left and the right context, these models can capture rich semantic meaning, disambiguate words, and achieve state-of-the-art performance in tasks like question answering, natural language inference, and sentiment analysis.1. bide

