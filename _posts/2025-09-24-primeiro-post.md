---
layout: post
title: "Meu primeiro post"
date: 2025-09-24 10:00:00 -0300
categories: [NLP, Attention]
---

Este √© o conte√∫do do meu primeiro post! üéâ

Aqui posso usar **Markdown** normalmente, adicionar imagens, links, listas, etc.

# Bem-vindo ao meu site

## Attention: a very simple explanation
**Learn attention with intuition, simple examples, and basic math!**

In 2017, Google researchers introduced the Transformer model in their now-famous paper
‚ÄúAttention Is All You Need.‚Äù This new approach marked a turning point in the field of
natural language processing (NLP). By rethinking how machines process language, it quickly
outperformed earlier models across key benchmarks such as machine translation, text
classification, text generation, and question answering. Its success laid the groundwork
for nearly all modern NLP systems.

> Paper: Vaswani et al. (2017), ‚ÄúAttention Is All You Need‚Äù.

## 1. Bidirectional models
When we read a sentence, we don't usually process each word on its own. Instead, we grasp
meaning by looking at how words connect to those around them. The context that comes before
and after a word often changes its sense completely, and this reliance on surrounding words
is what makes language so rich and flexible.

This ability to look at both sides of the sentence is exactly what defines bidirectional
models such as **BERT** and many derived models. By incorporating information from both the
left and the right context, these models can capture rich semantic meaning, disambiguate
words, and achieve state-of-the-art performance in tasks like question answering, natural
language inference, and sentiment analysis.
